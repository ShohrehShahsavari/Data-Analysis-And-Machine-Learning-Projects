from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt





ds= load_digits()


ds.feature_names


df= pd.DataFrame(ds.data, columns= ds.feature_names)
df['target'] = ds.target


df.info()


df.to_excel('Data_Digits.xlsx',index=False)





df.head()


pd.DataFrame(df.nunique(), columns= ['Number of unique values'])


def value_counts(df, column, n= 10):
    return pd.DataFrame(df[column].value_counts(), columns= ['count']).head(n)


for col in df.columns:
    print(value_counts(df, col, 3))
    print(10*'*')


columns_with_single_value=[]
for i in range(len(df.nunique())):
    if df.nunique()[i] == 1:
        columns_with_single_value.append(df.columns[i])
columns_with_single_value             


ds.images[22]


ds.data[22]


plt.imshow(ds.images[22], cmap= 'gray_r')
plt.title(f'target: {ds.target[22]}')
plt.xticks([])
plt.yticks([])
plt.tight_layout()


fig, axes= plt.subplots(nrows=4 , ncols=6 , figsize=(6,4))
for item in zip(axes.ravel(), ds.images, ds.target):
    ax, image, target = item
    ax.imshow(image, cmap='gray_r')# ((image, cmap=plt.cm.gray_r))
    ax.set_title(f"Label: {target}")
    ax.axis('off') # axes.set_xticks([])  # remove x-axis tick marks \n # axes.set_yticks([])  # remove y-axis tick marks
plt.tight_layout()





X= ds.data
y= ds.target
X_train, X_test, y_train, y_test= train_test_split(X, y, train_size= .75, random_state= 11)






Knn= KNeighborsClassifier()  
"""
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform')
"""
Knn.fit(X_train, y_train)


y_pred= Knn.predict(X_test)
y_test[:20]



y_pred[:20]


wrong_prediction= [(p, t) for (p, t) in zip(y_pred, y_test) if p != t]


wrong_prediction


# Using the predicted and expected arrays, calculate and display the prediction accuracy percentage
accuracy= (len(y_test) - len(wrong_prediction)) / len(y_test)
accuracy


X=df.drop(['target', 'pixel_0_0', 'pixel_4_0', 'pixel_4_7'], axis= 1)
y=df['target']
X_train, X_test, y_train, y_test= train_test_split(X, y, train_size= .75, random_state= 11)
Knn= KNeighborsClassifier()  
Knn.fit(X_train, y_train)
y_pred= Knn.predict(X_test)
# wrong_prediction= [(p, t) for (p, t) in zip(y_pred, y_test) if p != t]
# accuracy= (len(y_test) - len(wrong_prediction)) / len(y_test)
# accuracy


score=Knn.score(X_test, y_test)
score


from sklearn.metrics import confusion_matrix, classification_report

print(f'confusion_matric:\n{confusion_matrix(y_test, y_pred)}')

names= [str(name) for name in ds.target_names]
print(classification_report(y_test, y_pred, target_names= names))


# K-Fold Cross-Validation
from sklearn.model_selection import KFold, cross_val_score

kfold= KFold(n_splits= 10, random_state= 11, shuffle= True)
scores= cross_val_score(estimator= Knn, X= X, y=y, cv= kfold)
scores





from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB


estimators= {'KNeighborsClassifier': KNeighborsClassifier(), 
            'SVC': SVC(gamma= 'scale'),
            'GaussianNB': GaussianNB()}


for estimator_name, estimator_object in estimators.items():
    kfold= KFold(n_splits= 10, random_state= 11, shuffle= True)
    scores= cross_val_score(estimator=estimator_object , X=X, y=y, cv= kfold)
    # print(f'{estimator_name}: ' + 
    #       f'mean accuracy = {scores.mean():0.2%}; ' + 
    #       f'standard deviation = {scores.std():0.2%}')

    print(f'{estimator_name:>20}: ' +  
          f'mean accuracy={scores.mean():.2%}; ' +
          f'standard deviation={scores.std():.2%}')





for k in range(1, 20 ,2):
    kfold= KFold(n_splits= 10, random_state= 11, shuffle= True)
    scores= cross_val_score(estimator= KNeighborsClassifier(n_neighbors= k), X= X, y= y, cv= kfold)
    print(f'{k:<2}: ' + f'mean accuracy= {scores.mean():.2%}; ' + f'Standard deviation= {scores.std():.2%}')









