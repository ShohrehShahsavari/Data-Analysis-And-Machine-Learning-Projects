import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split





df= pd.read_csv('E:/Python Projects/shohreh/Shohreh_GitHub_Repository/Data-Analysis-And-Machine-Learning-Projects/5. SkLearn Datasets/Sklearn_CaliforniaHousingDataset/housing.csv')
df.info()


df['ocean_proximity'].value_counts()


from sklearn.preprocessing import LabelEncoder

le= LabelEncoder()
df['ocean_proximity']= le.fit_transform(df['ocean_proximity'])
le.classes_


df.describe().T





df.hist(bins=50, figsize= (20,15))
plt.show()


numeric_columns= df.select_dtypes(include=['number']).columns
numeric_columns
ncols=3
nrows= round(len(numeric_columns) / ncols)
fig, axes=plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,15))
fig.suptitle('Hist plot for all columns')
axes= axes.flatten()
for i , col in enumerate(numeric_columns):
    sns.histplot(df[col], bins=50, edgecolor='black', kde= True, ax= axes[i])

plt.tight_layout()
plt.show()


df.plot(kind= 'scatter', x='longitude' ,y='latitude', alpha=0.1, s=df["population"]/100, label="population", c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True)
plt.legend()





corr_matrix= df.corr()
corr_matrix['median_house_value'].sort_values()


# from Pandas.tools.plotting import scatter_matrix
from pandas.plotting import scatter_matrix
column=['median_house_value', 'median_income', 'housing_median_age', 'total_rooms']
scatter_matrix(df[column], figsize=(12,8))


df.plot(kind= 'scatter', y='median_house_value', x='median_income', alpha=0.1)


df['bedrooms_per_rooms']= df['total_bedrooms'] / df['total_rooms']
df['population_per_households']= df['population'] / df['households']
df['rooms_per_households']= df['total_rooms'] / df['households']
corr_matrix= df.corr()
corr_matrix['median_house_value'].sort_values()


# fill missing value for total_bedrooms

#df['total_bedrooms']= df['total_bedrooms'].fillna(df['total_bedrooms'].median())

df['total_bedrooms'] = df.groupby('total_rooms')['total_bedrooms'].transform(lambda x: x.fillna(x.median()))
df['total_bedrooms'] = df.groupby('median_house_value')['total_bedrooms'].transform(lambda x: x.fillna(x.median()))
df['total_bedrooms'] = df.groupby('ocean_proximity')['total_bedrooms'].transform(lambda x: x.fillna(x.median()))
df.info()


# fill missing value use Sklearn.impute
from sklearn.impute import SimpleImputer
imputer= SimpleImputer(strategy= 'median')
df['total_bedrooms'] = imputer.fit_transform(df[['total_bedrooms']])
df.info()

## fill missing value use Sklearn.impute for all dataset
#df=imputer.fit(df)





from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer


num_columns= df.select_dtypes(include=['number']).columns.tolist()
cat_columns= ['ocean_proximity']

pipeline_num= Pipeline([
    ('imputer',SimpleImputer(strategy='median')),
     ('std_scal',StandardScaler())
])
pipeline_cat= Pipeline([
     ('label_Encoder',OrdinalEncoder())
])
pipeline_full= ColumnTransformer([
    ('cat_pip', pipeline_cat, cat_columns),
    ('num_pip', pipeline_num, num_columns)
])

df_transform= pipeline_full.fit_transform(df)
df=pd.DataFrame(df_transform, columns= num_columns + cat_columns)
df.head()





X= df.drop(columns=['median_house_value'], axis=1)
y= df['median_house_value']


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=.8, random_state=42)





from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


model= LinearRegression()
model.fit(X_train, y_train)
y_pred= model.predict(X_test)
model.score(X_test, y_test)
print(f'MSE= {mean_squared_error(y_test, y_pred):.4f}\nR2= {r2_score(y_test, y_pred):.4f}')
np.sqrt(mean_squared_error(y_test, y_pred))





from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

model=DecisionTreeRegressor()
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
model.score(X_test, y_test)
print(f'MSE= {mean_squared_error(y_test, y_pred):.4f}\nR2= {r2_score(y_test, y_pred):.4f}')
np.sqrt(mean_squared_error(y_test, y_pred))






from sklearn.ensemble import RandomForestRegressor

model= RandomForestRegressor()
model.fit(X_train, y_train)
p_pred= model.predict(X_test)
model.score(X_test, y_test)
print(f'MSE= {mean_squared_error(y_test, y_pred):.4f}\nR2= {r2_score(y_test, y_pred):.4f}')
np.sqrt(mean_squared_error(y_test, y_pred))





from sklearn.svm import SVR
#from sklearn.metrics import mean_squared_error, r2_score

model= SVR()#(kernel='linear', C=100, gamma=0.1, epsilon=0.1)
model.fit(X_train, y_train)
p_pred= model.predict(X_test)
model.score(X_test, y_test)





from sklearn.model_selection import cross_val_score
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression


estimators= {
    'LinearRegression' : LinearRegression(),
    'DecisionTreeRegressor' : DecisionTreeRegressor(),
    'RandomForestRegressor' : RandomForestRegressor(),
    'SVR' : SVR()
}
for estimator_name, estimator_object in estimators.items():
    scores= cross_val_score(estimator=estimator_object, X=X, y=y, cv=2, scoring="r2")
    #print(f'{estimator_name:>20} :  Mean accuracy= {scores.mean():.4f}\n\t\t\tStandar deviation= {scores.std():.4f}')
    print(f'{estimator_name:>20} : Mean RÂ² = {scores.mean():.4f}\n\t\t\tStandard deviation = {scores.std():.4f}')
    


from sklearn.externals import joblib

joblib.dump(my_model, "my_model.pkl")
# and later...
my_model_loaded = joblib.load("my_model.pkl")





from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

params=[{'n_estimators' : [3,10,30], 'max_features' : [2,4,6,8]},
       {'bootstrap' : [False], 'n_estimators' : [3,10], 'max_features' : [2,4,6]}]
model= RandomForestRegressor()
gridSearch= GridSearchCV(model, params, cv=10, scoring= 'r2')
gridSearch.fit(X, y)
print(gridSearch.best_params_)
print(gridSearch.best_score_)
print(gridSearch.best_estimator_)
print(randomSearch.best_estimator_.feature_importances_)





from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor

params=[{'n_estimators' : [3,10,30], 'max_features' : [2,4,6,8]},
       {'bootstrap' : [False], 'n_estimators' : [3,10], 'max_features' : [2,4,6]}]
model= RandomForestRegressor()
randomSearch= RandomizedSearchCV(model, params, cv=2, scoring= 'r2')
randomSearch.fit(X, y)
print(randomSearch.best_params_)
print(randomSearch.best_score_)
print(randomSearch.best_estimator_)
print(randomSearch.best_estimator_.feature_importances_)


from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV

params= {
    'kernel' : ['linear', 'rbf'],
    'C' : [1, 3, 10, 30, 100, 300],
    'gamma' : [0.01, 0.03, 0.1, 0.3, 1.0, 3.0],
    'epsilon' : [0.1, 0.2]
}

model= SVR()
randomSearch= RandomizedSearchCV(model, params, cv=2, scoring= 'r2')
randomSearch.fit(X, y)
print(randomSearch.best_params_)
print(randomSearch.best_score_)
print(randomSearch.best_estimator_)



