


import pandas as pd
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeRegressor





ds = datasets.load_diabetes()
ds.feature_names
df=pd.DataFrame(data = ds.data, columns= ds.feature_names)
df['target']= ds.target
df.info()


# df.to_excel('Data.xlsx',index=False)


df.head(5)





# ncols= 3
# nrows= round(len(df.columns) / ncols)
# fig, axes = plt.subplots(nrows=nrows ,ncols=ncols, figsize= (15, 3*nrows))
# for item in zip(axes.ravel(), df.columns):
#     ax, column= item
#     sns.boxplot(x= df[column], ax= ax, color='skyblue')
# plt.tight_layout()
# plt.show()


ncols= 3
nrows= round(len(df.columns) / ncols)
fig, axes = plt.subplots(nrows=nrows ,ncols=ncols, figsize= (15, 3*nrows))
fig.suptitle('Boxplot for all columns', fontsize = 16, y= 1.0)
axes = axes.flatten()
for i, col in enumerate(df.columns):
  sns.boxplot(x=df[col], ax=axes[i], color='skyblue')
  # axes[i].set_xlabel(f'{col}')
  # axes[i].set_ylabel('Count')

# Hide unused subplots
for j in range(i+1, len(axes)):
  axes[j].axis('off')

plt.tight_layout()
plt.show()


ncols= 2
nrows= round(len(df.columns) / ncols)
fig, axes = plt.subplots(nrows=nrows, ncols= ncols, figsize=(15,3*nrows))
fig.suptitle('scatter plot for all columns', fontsize=16, y=1.02)
axes= axes.flatten()
for i, col in enumerate(df.columns):  # for i , col in enumerate(ds.feature_names): then you can escape if
  if col!= 'target':
    sns.scatterplot(x= df[col], y=df['target'], ax=axes[i], hue= df['target'], palette= 'cool', legend= False)
    axes[i].set_title(f'{col} vs target')

for j in range(i,len(axes)):
  axes[j].axis('off')

plt.tight_layout()
plt.show()


ncols=3
nrows= round(len(df.columns) / ncols)
fig ,axes= plt.subplots(nrows= nrows, ncols=ncols, figsize= (15, 3*nrows))
fig.suptitle('historam plot for all columns', fontsize= 16, y=1.02)
axes= axes.flatten()
for i, col in enumerate(df.columns):
  sns.histplot(df[col], bins=50, edgecolor='black', kde= True, ax= axes[i])

for j in range(i+1 , len(axes)):
  axes[j].axis('off')

plt.tight_layout()
plt.show()


# triangle heatmap
# fig=plt.subplots(figsize = (15,7))
# corr=df.corr(numeric_only=True)
# mask= np.triu(np.ones_like(corr, dtype= bool))   # mask= np.triu(corr)
# sns.heatmap(corr, annot = True, square= False, fmt = '.2f', mask= mask)
# plt.title('Heatmap for data')
# plt.show()


fig=plt.subplots(figsize = (15,7))
sns.heatmap(df.corr(numeric_only= True), annot = True, square= False, fmt = '.2f')
plt.title('Heatmap for data')
plt.show()





from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split


# Generate data for LinearRegression/single (bmi) variable
# X= np.array([df['bmi']]).reshape(-1,1)
# y= np.array(df['target'])

X= df[['bmi']]
y= df['target']

# Generate Model
model= LinearRegression()
model.fit(X,y)
print(f"Equation is: y = {model.intercept_:.4f} + {(model.coef_)[0]:.4f} * BMI")

# Evaluate Model
y_pred= model.predict(X)
mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")



# Plot model
plt.scatter(X, y, color='blue')
plt.plot(X, y_pred, color= 'red')
plt.xlabel('BMI')
plt.ylabel('Target')
plt.title('Single variable Linear Regression')
plt.legend(['Real data', 'Estimated Line'])
plt.grid(True)
plt.show()





X= df[['bmi']]
y= df['target']

X_train, X_test, y_train, y_test= train_test_split(X, y, random_state=11 , train_size= 0.7)
model= LinearRegression()
model.fit(X_train, y_train)
print(f'For X= bmi if Equation is y= mx + b => m = {(model.coef_)[0]:.4f} and b = {model.intercept_:.4f}')
y_pred= model.predict(X_test)
# for p, t in zip(y_pred[::5], y_test[::5]):
#     print(f'Prediction: {p:.4f} => Except: {t:.4f}')
axes= sns.scatterplot(data= df, x='bmi', y= 'target', hue= 'target', palette= 'winter', legend=True)# ['Real data', 'Estimated Line'])
line= plt.plot(X_test, y_pred, color= 'red')


from sklearn.metrics import mean_squared_error, r2_score
best_score= -1
best_col= None
for col in ds.feature_names:
    X= df[col].values.reshape(-1,1)
    y= df['target']
    X_train, X_test, y_train, y_test= train_test_split(X, y, random_state=11 , train_size= 0.8)
    model= LinearRegression()
    model.fit(X_train, y_train)
    y_pred= model.predict(X_test)
    #r2_score= r2_score(y_test, y_pred)
    r2_score= model.score(X_test, y_test)
    print(f'if X= {col} Equation is y= mx + b => m = {(model.coef_)[0]:.4f} and b = {model.intercept_:.4f}\nMSE= {mean_squared_error(y_test, y_pred):.4f} r2_score= {r2_score}')
    if r2_score > best_score:
        best_col= col
        best_score= r2_score
        
print(f'best col for sigle variable LinearRegression= ***{best_col}***, best score= ***{best_score:.4f}***')





# pd.set_option('precision', 4)
# pd.set_option('max_columns', 10)
# pd.set_option('display.width', None)




# load datasetds= datasets.load_diabetes()
df= pd.DataFrame(data= ds.data, columns=ds.feature_names)
df['target']= pd.Series(ds.target)
df.head()


X= df.drop(['target'], axis=1) # X= ds.data
y= df['target'] # y= ds.target
X_train, X_test, y_train, y_test= train_test_split(X, y, random_state= 11, train_size=.7)

model= LinearRegression()
model.fit(X_train, y_train)
y_pred= model.predict(X_test)
mse= mean_squared_error(y_test, y_pred)
r2= model.score(X_test, y_test)# r2= r2_score(y_test, y_pred)
print(f'MSE= {mse:.4f} r2_score= {r2}')
print(f'Eguation is: y(target) = ({(model.coef_)[0]:.4f})age + ({(model.coef_)[1]:.4f})sex +({(model.coef_)[2]:.4f})bmi+' + 
        f'({(model.coef_)[3]:.4f})bp +({(model.coef_)[4]:.4f})s1 +({(model.coef_)[5]:.4f})s2 +({(model.coef_)[6]:.4f})s3 +'+
        f'({(model.coef_)[7]:.4f})s4 +({(model.coef_)[8]:.4f})s5 +({(model.coef_)[9]:.4f})s6')
#model.score(X_test, y_test) => show r2_score





df_vis=pd.DataFrame()
df_vis['Expected']= pd.Series(y_test)
df_vis['Predicted']= pd.Series(y_pred)
figure= plt.figure(figsize=(5,5))
axes= sns.scatterplot(data= df_vis, x= 'Expected', y= 'Predicted', hue= 'Predicted', legend= False, palette= 'cool')
start= min(df_vis['Expected'].min(), df_vis['Predicted'].min())
end= max(df_vis['Expected'].max(), df_vis['Predicted'].max())
line= plt.plot([start, end], [start, end], 'k--')





# Normal equation:  Ax=b,  AT.Ax=AT.b.

def normal_equation (X,y):
  return np.linalg.inv(X.T @ X) @ X.T @ y

def mean_squared_error (y_true, y_predict):
  return np.mean((y_true - y_predict) ** 2)

def r2_score(y_true, y_pred):
  SS_Residual= np.sum((y_true - y_pred) ** 2)
  SS_Total= np.sum((y_true - np.mean(y_true)) ** 2)
  return 1 - (SS_Residual / SS_Total)

# Generate data for LinearRegression/single (bmi) variable
X= np.array([df['bmi']]).reshape(-1,1)
y= np.array(df['target'])

theta= normal_equation(X,y)

y_pred=np.dot(X,theta)

mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")

# Plot model
plt.scatter(X, y, color='blue')
plt.plot(X, y_pred, color= 'red')
plt.xlabel('BMI')
plt.ylabel('target')
plt.title('Single variable Normal Equation')
plt.legend(['Real data', 'Estimated Line'])
plt.grid(True)
plt.show()






def hypothesis (theta,X):
  return np.dot(X, theta)

def cost_fuction(theta, X, y):
  m= len(y)
  J=np.sum((hypothesis(theta,X) - y) ** 2) / (2 * m)
  return J

def Gradient_descent(theta, X, y, learning_rate, iterations):
  m=len(y)
  cost_history=np.zeros(iterations)
  for i in range(iterations):
    gradient= np.dot(X.T,(hypothesis(theta,X) - y)) / m
    theta= theta - (learning_rate * gradient)
    cost_history[i]= cost_fuction(theta, X, y)
  return theta, cost_history

def mean_squared_error (y_true, y_predict):
  return np.mean((y_true - y_predict) ** 2)

def r2_score(y_true, y_pred):
  SS_Residual= np.sum((y_true - y_pred) ** 2)
  SS_Total= np.sum((y_true - np.mean(y_true)) ** 2)
  return 1 - (SS_Residual / SS_Total)

# Generate data for LinearRegression/single (bmi) variable
# X= np.array([df['bmi']]).reshape(-1,1)
# y= np.array(df['target'])
X= df[['bmi']]
y=df['target']

initia_theta= np.zeros(X.shape[1])
learning_rate= .0001
iterations= 1000

theta, cost_history = Gradient_descent(initia_theta, X, y, learning_rate, iterations)


y_pred=hypothesis(theta, X)

mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)

print(f" MSE = {mse}")
print(f" R2 = {r2}")

# Plot model
plt.scatter(X, y, color='blue')
plt.plot(X, y_pred, color= 'red')
plt.xlabel('BMI')
plt.ylabel('Target')
plt.title('Single variable Gradient descent')
plt.legend(['Real data', 'Estimated Line'])
plt.grid(True)
plt.show()





X= df[['bmi', 's5']]
y= df['target']

# Generate Model
model= LinearRegression()
model.fit(X,y)
print(f"Equation is: y = {model.intercept_:.4f} + {(model.coef_)[0]:.4f} * BMI + {(model.coef_)[1]:.4f} * s5")

# Evaluate Model
y_pred= model.predict(X)
mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")

# Drew plot
fig= plt.figure()
ax= fig.add_subplot(111, projection= '3d')
ax.scatter(X['bmi'], X['s5'], y)
ax.scatter(X['bmi'], X['s5'], y_pred)
plt.show()





df.columns


from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

X= ds.data
y= ds.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state=42)
model= LinearRegression()
model.fit(X_train, y_train)

print(f"Equation is: y = {model.intercept_:.4f} + {(model.coef_)[0]:.4f} * AGE + {(model.coef_)[1]:.4f} * SEX + {(model.coef_)[2]:.4f} * MBI + {(model.coef_)[3]:.4f} * bp + {(model.coef_)[4]:.4f} * S1 + {(model.coef_)[5]:.4f} * S2 + {(model.coef_)[6]:.4f} * S3 + {(model.coef_)[7]:.4f} * S4 + {(model.coef_)[8]:.4f} * S5 + {(model.coef_)[9]:.4f} * S6")

y_pred= model.predict(X_test)
mse= mean_squared_error(y_test, y_pred)
r2= r2_score(y_test, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")





from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge

X= ds.data
y= ds.target

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size = 0.8 , random_state= 42)

model = LinearRegression()
poly= PolynomialFeatures(2)
# poly= PolynomialFeatures(include_bias= False)
X_train_Poly= poly.fit_transform(X_train)
X_test_poly= poly.fit_transform(X_test)
model.fit(X_train_Poly, y_train)
y_pred=model.predict(X_test_poly)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")


from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge

X= ds.data
y= ds.target

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size = 0.75 , random_state= 42)

model = Pipeline([
    ('polynomial_features', PolynomialFeatures(degree=2)),
    ('standard_scaler', StandardScaler()),
    ('ridge_regression', Ridge())
])


model.fit(X_train, y_train)
y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")





X= df.drop('target', axis=1)
y= df[['target']]

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=42)

model= Ridge(alpha=0.2)
model.fit(X_train, y_train)

y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f'MSE: {mse}')
print(f'R2: {r2}')


X= df.drop('target', axis=1)
y= df[['target']]

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=42)

model= Lasso(alpha=0.1)
model.fit(X_train, y_train)

y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f'MSE: {mse}')
print(f'R2: {r2}')


X= df.drop('target', axis=1)
y= df[['target']]

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=1)

model= ElasticNet(alpha=0.0001, l1_ratio=0.1)
model.fit(X_train, y_train)

y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f'MSE: {mse}')
print(f'R2: {r2}')





!pip install scikit_optimize


from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from skopt import BayesSearchCV

X= df.drop('target', axis=1)
y= df[['target']]
X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=1)
model= ElasticNet()
params= {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
         'l1_ratio': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
         }

# # Grid search
# grid_search= GridSearchCV(estimator= model, param_grid= params, scoring='r2', cv=6)
# grid_search.fit(X_train, y_train)
# print(f'Best for GridSearch: {grid_search.best_estimator_}')
# print(f'Best score for GridSearch: {grid_search.best_score_}')

# # Random search
# random_search= RandomizedSearchCV(estimator= model, param_distributions= params, n_iter= 20, scoring='r2', cv=6, random_state= 42)
# random_search.fit(X_train, y_train)
# print(f'Best for RandomSearch: {random_search.best_estimator_}')
# print(f'Best score for RandomSearch: {random_search.best_score_}')

#Bayesian optimization
opt= BayesSearchCV(estimator= model, search_spaces= params, n_iter= 20, scoring= 'r2', cv= 6)
opt.fit(X_train, y_train)
print(f'Best for BayesSearch: {opt.best_estimator_}')
print(f'Best score for BayesSearch: {opt.best_score_}')





X= ds.data
y= ds.target
X_train, X_test, y_train, y_test= train_test_split(X, y, train_size= 0.7, random_state= 42)


model= DecisionTreeRegressor()
model.fit(X_train, y_train)
y_pred= model.predict(X_test)
mse= mean_squared_error(y_test, y_pred)
r2= r2_score(y_test, y_pred)
print(f'MSE: {mse:.4f}')
print(f'R2_score: {r2:.4f}')





from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor


X= df.drop(['target'], axis=1) # X= ds.data
y= df['target'] # y= ds.target

estimators= {
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'DecisionTreeRegressor': DecisionTreeRegressor(),
    'KNeighborsRegressor': KNeighborsRegressor()
}
for estimator_name, estimator_object in estimators.items():
    kfold= KFold(n_splits= 10, random_state= 11, shuffle= True)
    scores= cross_val_score(estimator= estimator_object, X= X, y= y, cv=kfold, scoring= 'r2')
    print(f'{estimator_name:>16}: ' + f'{scores.mean():.4f}')






