


import pandas as pd
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeRegressor





ds = datasets.load_diabetes()
ds.feature_names
df=pd.DataFrame(data = ds.data, columns= ds.feature_names)
df['target']= ds.target
df.info()


df.to_excel('Data.xlsx',index=False)


df.head(5)





ncols= 3
nrows= round(len(df.columns) / ncols)
fig, axes = plt.subplots(nrows=nrows ,ncols=ncols, figsize= (15, 3*nrows))
fig.suptitle('Boxplot for all columns', fontsize = 16, y= 1.0)
axes = axes.flatten()
for i, col in enumerate(df.columns):
  sns.boxplot(x=df[col], ax=axes[i], color='skyblue')
  # axes[i].set_xlabel(f'{col}')
  # axes[i].set_ylabel('Count')

# Hide unused subplots
for j in range(i+1, len(axes)):
  axes[j].axis('off')

plt.tight_layout()
plt.show()


ncols= 2
nrows= round(len(df.columns) / ncols)
fig, axes = plt.subplots(nrows=nrows, ncols= ncols, figsize=(15,3*nrows))
fig.suptitle('scatter plot for all columns', fontsize=16, y=1.02)
axes= axes.flatten()
for i, col in enumerate(df.columns):
  if col!= 'target':
    sns.scatterplot(x= df[col], y=df['target'], ax=axes[i])
    axes[i].set_title(f'{col} vs target')

for j in range(i,len(axes)):
  axes[j].axis('off')

plt.tight_layout()
plt.show()


ncols=3
nrows= round(len(df.columns) / ncols)
fig ,axes= plt.subplots(nrows= nrows, ncols=ncols, figsize= (15, 3*nrows))
fig.suptitle('historam plot for all columns', fontsize= 16, y=1.02)
axes= axes.flatten()
for i, col in enumerate(df.columns):
  sns.histplot(df[col], bins=50, edgecolor='black', kde= True, ax= axes[i])

for j in range(i+1 , len(axes)):
  axes[j].axis('off')

plt.tight_layout()
plt.show()


# triangle heatmap
# fig=plt.subplots(figsize = (15,7))
# corr=df.corr(numeric_only=True)
# mask= np.triu(np.ones_like(corr, dtype= bool))   # mask= np.triu(corr)
# sns.heatmap(corr, annot = True, square= False, fmt = '.2f', mask= mask)
# plt.title('Heatmap for data')
# plt.show()


fig=plt.subplots(figsize = (15,7))
sns.heatmap(df.corr(numeric_only= True), annot = True, square= False, fmt = '.2f')
plt.title('Heatmap for data')
plt.show()





from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


# Generate data for LinearRegression/single (bmi) variable
# X= np.array([df['bmi']]).reshape(-1,1)
# y= np.array(df['target'])

X= df[['bmi']]
y= df['target']

# Generate Model
model= LinearRegression()
model.fit(X,y)
print(f"Equation is: y = {model.intercept_:.4f} + {(model.coef_)[0]:.4f} * BMI")

# Evaluate Model
y_pred= model.predict(X)
mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")



# Plot model
plt.scatter(X, y, color='blue')
plt.plot(X, y_pred, color= 'red')
plt.xlabel('BMI')
plt.ylabel('Target')
plt.title('Single variable Linear Regression')
plt.legend(['Real data', 'Estimated Line'])
plt.grid(True)
plt.show()





# Normal equation:  Ax=b,  AT.Ax=AT.b.

def normal_equation (X,y):
  return np.linalg.inv(X.T @ X) @ X.T @ y

def mean_squared_error (y_true, y_predict):
  return np.mean((y_true - y_predict) ** 2)

def r2_score(y_true, y_pred):
  SS_Residual= np.sum((y_true - y_pred) ** 2)
  SS_Total= np.sum((y_true - np.mean(y_true)) ** 2)
  return 1 - (SS_Residual / SS_Total)

# Generate data for LinearRegression/single (bmi) variable
X= np.array([df['bmi']]).reshape(-1,1)
y= np.array(df['target'])

theta= normal_equation(X,y)

y_pred=np.dot(X,theta)

mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")

# Plot model
plt.scatter(X, y, color='blue')
plt.plot(X, y_pred, color= 'red')
plt.xlabel('BMI')
plt.ylabel('target')
plt.title('Single variable Normal Equation')
plt.legend(['Real data', 'Estimated Line'])
plt.grid(True)
plt.show()






def hypothesis (theta,X):
  return np.dot(X, theta)

def cost_fuction(theta, X, y):
  m= len(y)
  J=np.sum((hypothesis(theta,X) - y) ** 2) / (2 * m)
  return J

def Gradient_descent(theta, X, y, learning_rate, iterations):
  m=len(y)
  cost_history=np.zeros(iterations)
  for i in range(iterations):
    gradient= np.dot(X.T,(hypothesis(theta,X) - y)) / m
    theta= theta - (learning_rate * gradient)
    cost_history[i]= cost_fuction(theta, X, y)
  return theta, cost_history

def mean_squared_error (y_true, y_predict):
  return np.mean((y_true - y_predict) ** 2)

def r2_score(y_true, y_pred):
  SS_Residual= np.sum((y_true - y_pred) ** 2)
  SS_Total= np.sum((y_true - np.mean(y_true)) ** 2)
  return 1 - (SS_Residual / SS_Total)

# Generate data for LinearRegression/single (bmi) variable
# X= np.array([df['bmi']]).reshape(-1,1)
# y= np.array(df['target'])
X= df[['bmi']]
y=df['target']

initia_theta= np.zeros(X.shape[1])
learning_rate= .0001
iterations= 1000

theta, cost_history = Gradient_descent(initia_theta, X, y, learning_rate, iterations)


y_pred=hypothesis(theta, X)

mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)

print(f" MSE = {mse}")
print(f" R2 = {r2}")

# Plot model
plt.scatter(X, y, color='blue')
plt.plot(X, y_pred, color= 'red')
plt.xlabel('BMI')
plt.ylabel('Target')
plt.title('Single variable Gradient descent')
plt.legend(['Real data', 'Estimated Line'])
plt.grid(True)
plt.show()





X= df[['bmi', 's5']]
y= df['target']

# Generate Model
model= LinearRegression()
model.fit(X,y)
print(f"Equation is: y = {model.intercept_:.4f} + {(model.coef_)[0]:.4f} * BMI + {(model.coef_)[1]:.4f} * s5")

# Evaluate Model
y_pred= model.predict(X)
mse= mean_squared_error(y, y_pred)
r2= r2_score(y, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")

# Drew plot
fig= plt.figure()
ax= fig.add_subplot(111, projection= '3d')
ax.scatter(X['bmi'], X['s5'], y)
ax.scatter(X['bmi'], X['s5'], y_pred)
plt.show()





df.columns


from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

X= ds.data
y= ds.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state=42)
model= LinearRegression()
model.fit(X_train, y_train)

print(f"Equation is: y = {model.intercept_:.4f} + {(model.coef_)[0]:.4f} * AGE + {(model.coef_)[1]:.4f} * SEX + {(model.coef_)[2]:.4f} * MBI + {(model.coef_)[3]:.4f} * bp + {(model.coef_)[4]:.4f} * S1 + {(model.coef_)[5]:.4f} * S2 + {(model.coef_)[6]:.4f} * S3 + {(model.coef_)[7]:.4f} * S4 + {(model.coef_)[8]:.4f} * S5 + {(model.coef_)[9]:.4f} * S6")

y_pred= model.predict(X_test)
mse= mean_squared_error(y_test, y_pred)
r2= r2_score(y_test, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")





from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge

X= ds.data
y= ds.target

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size = 0.8 , random_state= 42)

model = LinearRegression()
poly= PolynomialFeatures(2)
# poly= PolynomialFeatures(include_bias= False)
X_train_Poly= poly.fit_transform(X_train)
X_test_poly= poly.fit_transform(X_test)
model.fit(X_train_Poly, y_train)
y_pred=model.predict(X_test_poly)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")


from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge

X= ds.data
y= ds.target

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size = 0.75 , random_state= 42)

model = Pipeline([
    ('polynomial_features', PolynomialFeatures(degree=2)),
    ('standard_scaler', StandardScaler()),
    ('ridge_regression', Ridge())
])


model.fit(X_train, y_train)
y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f" MSE = {mse}")
print(f" R2 = {r2}")





X= df.drop('target', axis=1)
y= df[['target']]

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=42)

model= Ridge(alpha=0.2)
model.fit(X_train, y_train)

y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f'MSE: {mse}')
print(f'R2: {r2}')


X= df.drop('target', axis=1)
y= df[['target']]

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=42)

model= Lasso(alpha=0.1)
model.fit(X_train, y_train)

y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f'MSE: {mse}')
print(f'R2: {r2}')


X= df.drop('target', axis=1)
y= df[['target']]

X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=1)

model= ElasticNet(alpha=0.0001, l1_ratio=0.1)
model.fit(X_train, y_train)

y_pred=model.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
r2=r2_score(y_test, y_pred)
print(f'MSE: {mse}')
print(f'R2: {r2}')





!pip install scikit_optimize


from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from skopt import BayesSearchCV

X= df.drop('target', axis=1)
y= df[['target']]
X_train, X_test, y_train, y_test= train_test_split(X, y, train_size=0.2, random_state=1)
model= ElasticNet()
params= {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
         'l1_ratio': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
         }

# # Grid search
# grid_search= GridSearchCV(estimator= model, param_grid= params, scoring='r2', cv=6)
# grid_search.fit(X_train, y_train)
# print(f'Best for GridSearch: {grid_search.best_estimator_}')
# print(f'Best score for GridSearch: {grid_search.best_score_}')

# # Random search
# random_search= RandomizedSearchCV(estimator= model, param_distributions= params, n_iter= 20, scoring='r2', cv=6, random_state= 42)
# random_search.fit(X_train, y_train)
# print(f'Best for RandomSearch: {random_search.best_estimator_}')
# print(f'Best score for RandomSearch: {random_search.best_score_}')

#Bayesian optimization
opt= BayesSearchCV(estimator= model, search_spaces= params, n_iter= 20, scoring= 'r2', cv= 6)
opt.fit(X_train, y_train)
print(f'Best for BayesSearch: {opt.best_estimator_}')
print(f'Best score for BayesSearch: {opt.best_score_}')





X= ds.data
y= ds.target
X_train, X_test, y_train, y_test= train_test_split(X, y, train_size= 0.7, random_state= 42)


model= DecisionTreeRegressor()
model.fit(X_train, y_train)
y_pred= model.predict(X_test)
mse= mean_squared_error(y_test, y_pred)
r2= r2_score(y_test, y_pred)
print(f'MSE: {mse:.4f}')
print(f'R2_score: {r2:.4f}')



