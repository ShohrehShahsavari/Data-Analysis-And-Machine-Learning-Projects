


from google.colab import drive
drive.mount("/content/drive", force_remount=True)





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re





df=pd.read_csv("/content/drive/MyDrive/Data/Dataset_Used/Uncleaned_DS_jobs.csv")
df.head(2)


df.info()





df.shape


df.isnull().sum()


df.columns


pd.DataFrame(df.nunique(), columns=["Number of Unique values"])





def value_count(df,column):
  return pd.DataFrame(df[column].value_counts()).head(10)


value_count(df,column=['Job Title'])


value_count(df,column=['Salary Estimate'])


value_count(df,column=['Rating'])


value_count(df,column=['Company Name'])


value_count(df,column=['Location'])


value_count(df,column=['Headquarters'])


value_count(df,column=['Size'])


value_count(df,column=['Founded'])


value_count(df,column=['Type of ownership'])


value_count(df,column=['Industry'])


value_count(df,column=['Sector'])


value_count(df,column=['Revenue'])


value_count(df,column=['Competitors'])


df.duplicated().sum()








# Drop the 'Competitors', 'index', 'Headquarters' and 'Job Description' columns as they are not needed.

df.drop(columns=['index','Competitors', 'Job Description'], inplace=True)
df.info()






# Define the mapping for ranges
bins = [-2, 0, 1, 2, 3, 4, 5]
labels = ['Very poor', 'Poor', 'Lower Medium', 'Upper Medium', 'High', 'Excellent']

# Map values in column 'Rating' using pd.cut
df['Rating'] = pd.cut(df['Rating'], bins=bins, labels=labels)



value_count(df,column=['Rating'])





# Regex pattern to extract salary ranges
pattern = r'\$(\d+)K-\$(\d+)K'

# Iterate over rows and calculate the mean salary
for i, row in df.iterrows():
  text=row['Salary Estimate']
  matches = re.findall(pattern, text)  # Find all matches using the regex pattern
  if matches:  # Check if matches were found
    lower_salary = int(matches[0][0])  # Extract the lower salary
    upper_salary = int(matches[0][1])  # Extract the upper salary
    mean_salary = (lower_salary + upper_salary) / 2  # Calculate the mean salary
    df.at[i, 'Salary Estimate'] = mean_salary  # Update the DataFrame with the mean salary


# change type of 'Salary Estimate'
df['Salary Estimate']= df['Salary Estimate'].astype('float')

# change Salary Estimate column name
df.rename(columns={'Salary Estimate': 'Mean of Salary Estimate (M$)'}, inplace=True)





# Regex pattern to extract Company Name
pattern = r'[A-Za-z\s]+'
# Function to extract alphabetic content from a string
def extract_alphabetic(text):
    matches = re.findall(pattern, text)  # Find all matches using the regex pattern
    if matches:  # Check if matches were found
        return matches[0].strip()  # Return the first match and strip whitespace
    return text  # Return the original text if no match is found

# Apply the function to the 'Company Name' column
df['Company Name'] = df['Company Name'].apply(extract_alphabetic)




df['Company Name']





def split_location(text):
    """
    Splits the 'Location' text by comma and returns the first part (city name).
    Handles cases where the text is None or does not contain a comma.
    """
    if pd.isna(text) or not isinstance(text, str):  # Handle missing or non-string values
        return text
    return text.split(',')[0].strip()  # Split by comma, take the first part, and strip whitespace

df['Location']=df['Location'].apply(split_location)





# Mapping dictionary
mapping= {
    '1 to 50 employees' : 1,
    '51 to 200 employees': 2,
    '201 to 500 employees' : 3,
    '501 to 1000 employees' : 4,
    '1001 to 5000 employees' : 5,
    '5001 to 10000 employees': 6 ,
    '10000+ employees' : 7,
    'Unknown' : np.NaN}
# Apply mapping to the 'Size' column
df['Size']= df['Size'].map(mapping)
value_count(df,column=['Size'])


# Replace -1 with NaN
df.replace(-1, pd.NA, inplace= True)
df.info()


import pandas as pd

# Load the dataset
df = pd.read_csv('Uncleaned_DS_jobs.csv')

# Display the first few rows of the dataset
print("Initial dataset:")
print(df.head())

# 1. Handle Missing Values
# Check for missing values in each column
print("\nMissing values in each column:")
print(df.isnull().sum())

# Fill missing values in numeric columns with the mean
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Fill missing values in categorical columns with the mode
categorical_columns = df.select_dtypes(include=['object']).columns
df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])

# 2. Remove Duplicates
# Check for duplicate rows
print("\nNumber of duplicate rows before removal:", df.duplicated().sum())

# Remove duplicate rows
df = df.drop_duplicates()

# 3. Standardize Data Formats
# Convert 'Salary Estimate' to numeric by removing non-numeric characters
df['Salary Estimate'] = df['Salary Estimate'].str.replace('[^\d]', '', regex=True).astype(float)

# Convert 'Founded' to datetime format
df['Founded'] = pd.to_datetime(df['Founded'], errors='coerce')

# 4. Correct Inconsistencies
# Standardize 'Location' column by stripping extra spaces and converting to uppercase
df['Location'] = df['Location'].str.strip().str.upper()

# 5. Remove Irrelevant Columns
# Drop columns that are not needed for analysis
df = df.drop(columns=['Competitors', 'Headquarters'])

# 6. Save the Cleaned Dataset
df.to_csv('Cleaned_DS_jobs.csv', index=False)

# Display the first few rows of the cleaned dataset
print("\nCleaned dataset:")
print(df.head())
